{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wf-WqAWWpeUr"
   },
   "source": [
    "# **Judul dan Pembahasan**\n",
    "# Kompilasi Materi Sesi 7: Analisis Sentimen (BoW & TF-IDF)\n",
    "\n",
    "**Nama:** Heriswaya\n",
    "**Tugas:** Sesi 7 PRIMA MAGANG PTKI\n",
    "\n",
    "Notebook ini adalah gabungan (kompilasi) dari semua materi dan kode latihan Sesi 7.\n",
    "\n",
    "Di sini, saya akan mempraktikkan dua metode fundamental untuk **Analisis Sentimen**:\n",
    "1.  **Metode 1: Bag-of-Words (BoW)**, yang fokus pada frekuensi kemunculan kata.\n",
    "2.  **Metode 2: TF-IDF (Term Frequency-Inverse Document Frequency)**, yang fokus pada seberapa \"penting\" sebuah kata.\n",
    "\n",
    "Kedua metode ini akan digunakan untuk melatih model Naive Bayes dalam mengklasifikasikan ulasan film sebagai 'Positif' atau 'Negatif'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5O-UgRx9pryB"
   },
   "source": [
    "## Bagian 1: Analisis Sentimen dengan Bag-of-Words (BoW)\n",
    "\n",
    "Pertama, saya akan membangun model klasifikasi sentimen menggunakan metode Bag-of-Words (BoW). Metode ini mengubah teks menjadi angka dengan cara menghitung frekuensi kemunculan setiap kata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oA0QB4ypzRq"
   },
   "source": [
    "### 1.1 Import Library (BoW)\n",
    "\n",
    "Saya akan mengimpor dua library utama dari Scikit-learn:\n",
    "* `CountVectorizer`: Ini adalah alat yang akan mengubah kalimat saya menjadi matriks angka berdasarkan hitungan kata (BoW).\n",
    "* `MultinomialNB`: Ini adalah algoritma Machine Learning (Naive Bayes) yang akan saya gunakan untuk melatih model klasifikasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0OJyAfqio-cR",
    "outputId": "2903c8ac-e369-4cce-e550-a3edc4fd3470"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import library untuk Bag-of-Words\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnaive_bayes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultinomialNB\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLibrary untuk BoW siap.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Import library untuk Bag-of-Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "print(\"Library untuk BoW siap.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVoyaMVyp35B"
   },
   "source": [
    "### 1.2 Menyiapkan Data Training (BoW)\n",
    "\n",
    "Saya akan menyiapkan data latih (`corpus`) dan labelnya (`y_labels`). Saya akan membuat data yang seimbang: 3 ulasan positif dan 3 ulasan negatif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ArK2cq6qp6PO",
    "outputId": "9e980223-7691-4577-cc28-742096e30011"
   },
   "outputs": [],
   "source": [
    "# 1. Data Training (BoW)\n",
    "# Saya beri nama corpus_bow agar tidak tertukar dengan latihan TF-IDF\n",
    "corpus_bow = [\n",
    "    # Positif\n",
    "    'Saya suka film ini',\n",
    "    'film ini sangat bagus dan saya suka',\n",
    "    'review film ini luar biasa bagus',\n",
    "\n",
    "    # Negatif\n",
    "    'Saya benci film itu',\n",
    "    'Saya tidak suka film ini', # Penting untuk menangkap \"tidak suka\"\n",
    "    'film ini jelek sekali'\n",
    "]\n",
    "\n",
    "# Label yang sesuai\n",
    "y_labels_bow = [\n",
    "    'Positif', 'Positif', 'Positif',\n",
    "    'Negatif', 'Negatif', 'Negatif'\n",
    "]\n",
    "\n",
    "print(f\"Data BoW: {len(corpus_bow)} ulasan, {len(y_labels_bow)} label.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6y6SEaz6p8Kn"
   },
   "source": [
    "### 1.3 Vektorisasi (BoW) dengan N-grams\n",
    "\n",
    "Sekarang, saya akan mengubah `corpus_bow` saya menjadi angka menggunakan `CountVectorizer`.\n",
    "\n",
    "Di sini saya akan menggunakan parameter `ngram_range=(1, 2)`. Ini adalah langkah penting. Artinya, model tidak hanya akan melihat kata satu per satu (unigram, 1-gram), tetapi juga pasangan kata yang berurutan (bigram, 2-gram). Ini sangat membantu model untuk memahami konteks, contohnya membedakan \"suka\" dan \"tidak suka\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p9_hjG0vp-BD",
    "outputId": "9aecade8-41bf-4e9c-e69a-b1866a78bb8d"
   },
   "outputs": [],
   "source": [
    "# 2. Vektorisasi (BoW)\n",
    "# Saya set ngram_range=(1, 2) untuk menangkap unigram dan bigram\n",
    "vectorizer_bow = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# Latih vectorizer dan ubah corpus menjadi matriks angka (BoW)\n",
    "X_bow = vectorizer_bow.fit_transform(corpus_bow)\n",
    "\n",
    "print(\"Vektorisasi BoW (dengan n-gram) selesai.\")\n",
    "# Untuk melihat hasilnya (opsional):\n",
    "# print(vectorizer_bow.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8n-oeZsIp_wT"
   },
   "source": [
    "### 1.4 Melatih Model (BoW)\n",
    "\n",
    "Setelah data saya menjadi angka (`X_bow`), saya bisa melatih model `MultinomialNB`.\n",
    "\n",
    "Saya akan memanggil fungsi `.fit()` dengan data latih (`X_bow`) dan labelnya (`y_labels_bow`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J0rBxuVjqCDg",
    "outputId": "624666af-adcf-4190-9a3c-954f5967c446"
   },
   "outputs": [],
   "source": [
    "# 3. Melatih Model (BoW)\n",
    "model_bow = MultinomialNB()\n",
    "model_bow.fit(X_bow, y_labels_bow)\n",
    "\n",
    "print(\"Model BoW berhasil dilatih!\")\n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tq1QtYcsqD9B"
   },
   "source": [
    "### 1.5 Uji Coba Model (BoW)\n",
    "\n",
    "Saatnya menguji model. Saya akan menyiapkan `review_baru`.\n",
    "\n",
    "Poin penting di sini: saya harus menggunakan vectorizer yang **SAMA** (`vectorizer_bow`) dan hanya memanggil `.transform()`, bukan `.fit_transform()`. Ini untuk memastikan data baru diproses dengan \"kamus\" yang sama dengan data latih.\n",
    "\n",
    "Setelah itu, saya gunakan `model_bow.predict()` untuk melihat hasilnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TjmZsoVuqGGY",
    "outputId": "1ebd86f7-4bee-4585-dd45-80df04959344"
   },
   "outputs": [],
   "source": [
    "# 4. Uji Coba Model (BoW)\n",
    "review_baru_bow = [\n",
    "    'Saya benci film ini', # Harusnya 'Negatif'\n",
    "    'film ini bagus dan saya suka' # Harusnya 'Positif'\n",
    "]\n",
    "\n",
    "# 5. Prediksi (BoW)\n",
    "# Gunakan .transform() saja\n",
    "X_baru_bow = vectorizer_bow.transform(review_baru_bow)\n",
    "\n",
    "# Lakukan prediksi\n",
    "prediksi_bow = model_bow.predict(X_baru_bow)\n",
    "\n",
    "print(\"Hasil Prediksi Model BoW:\")\n",
    "print(f\"Review: {review_baru_bow[0]} -> Prediksi: {prediksi_bow[0]}\")\n",
    "print(f\"Review: {review_baru_bow[1]} -> Prediksi: {prediksi_bow[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-zNwJVaqIUa"
   },
   "source": [
    "## Bagian 2: Analisis Sentimen dengan TF-IDF\n",
    "\n",
    "Sekarang saya akan mencoba metode kedua, yaitu **TF-IDF (Term Frequency-Inverse Document Frequency)**.\n",
    "\n",
    "Berbeda dengan BoW yang hanya menghitung frekuensi, TF-IDF mengukur \"seberapa penting\" sebuah kata dalam satu dokumen relatif terhadap keseluruhan dokumen (corpus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "te1GJ_QisZA5"
   },
   "source": [
    "### 2.1 Import Library (TF-IDF)\n",
    "\n",
    "Mirip seperti sebelumnya, tapi kali ini saya akan mengimpor `TfidfVectorizer`. Model klasifikasinya tetap sama, yaitu `MultinomialNB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IrcaBoGTsau5",
    "outputId": "ed5e21c4-16ab-4962-a856-2f69f69b8ced"
   },
   "outputs": [],
   "source": [
    "# Import library untuk TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB # (Sebenarnya sudah diimpor, tapi ini untuk kejelasan)\n",
    "\n",
    "print(\"\\nLibrary untuk TF-IDF siap.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EO7viMwoscN9"
   },
   "source": [
    "### 2.2 Menyiapkan Data Training (TF-IDF)\n",
    "\n",
    "Sesuai file Latihan 2, saya akan menggunakan dataset yang sedikit berbeda (lebih kecil) untuk mendemonstrasikan metode ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wu01RS3fsdra",
    "outputId": "9dacaef5-3815-4214-cdac-585605d535e2"
   },
   "outputs": [],
   "source": [
    "# 1. Data Training (TF-IDF)\n",
    "corpus_tfidf = [\n",
    "    'Saya suka film ini',\n",
    "    'Saya benci film itu',\n",
    "    'film ini sangat bagus dan saya suka'\n",
    "]\n",
    "y_labels_tfidf = ['Positif', 'Negatif', 'Positif']\n",
    "\n",
    "print(f\"Data TF-IDF: {len(corpus_tfidf)} ulasan, {len(y_labels_tfidf)} label.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MLSdL7Bsf63"
   },
   "source": [
    "### 2.3 Vektorisasi (TF-IDF) dengan Stopwords\n",
    "\n",
    "Saat membuat `TfidfVectorizer`, saya akan mencoba parameter yang berbeda. Kali ini, saya akan menggunakan `stop_words` untuk secara otomatis mengabaikan kata-kata umum dalam bahasa Indonesia (seperti 'saya', 'ini', 'itu', 'dan') yang tidak membawa banyak makna sentimen.\n",
    "\n",
    "Kemudian, saya akan memanggil `.fit_transform()` pada data `corpus_tfidf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUCGHZyOs7ox",
    "outputId": "d9259d81-b90c-4858-b3e8-2af3e2a4387e"
   },
   "outputs": [],
   "source": [
    "# 2. Vektorisasi (TF-IDF)\n",
    "# Saya hapus stopwords bahasa Indonesia secara manual\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=['saya', 'ini', 'itu', 'dan'])\n",
    "\n",
    "# Latih vectorizer dan ubah corpus menjadi matriks TF-IDF\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus_tfidf)\n",
    "\n",
    "print(\"Vektorisasi TF-IDF (dengan stop_words) selesai.\")\n",
    "# Untuk melihat hasilnya (opsional):\n",
    "# print(tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdk90PZQs9kO"
   },
   "source": [
    "### 2.4 Melatih Model (TF-IDF)\n",
    "\n",
    "Proses pelatihannya sama persis dengan BoW. Saya akan membuat instance `MultinomialNB` baru dan melatihnya menggunakan data TF-IDF (`X_tfidf`) dan labelnya (`y_labels_tfidf`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mca9WKPFs_Ou",
    "outputId": "100a0e9c-a0a4-47f3-a752-f9897dff9d53"
   },
   "outputs": [],
   "source": [
    "# 3. Melatih Model (TF-IDF)\n",
    "model_tfidf = MultinomialNB()\n",
    "model_tfidf.fit(X_tfidf, y_labels_tfidf)\n",
    "\n",
    "print(\"Model TF-IDF berhasil dilatih!\")\n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30acLbb4tAvE"
   },
   "source": [
    "### 2.5 Uji Coba Model (TF-IDF)\n",
    "\n",
    "Terakhir, saya akan menguji model TF-IDF ini. Saya akan menggunakan data uji yang sama dengan Latihan 2.\n",
    "\n",
    "Sama seperti BoW, saya wajib menggunakan `.transform()` (bukan `.fit_transform()`) pada `tfidf_vectorizer` yang sudah dilatih. Kemudian saya panggil `.predict()` pada `model_tfidf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDIm_HtZtCRe",
    "outputId": "2b946e0c-8288-4c9d-e5c0-6d60ec9856e6"
   },
   "outputs": [],
   "source": [
    "# 4. Uji Coba Model (TF-IDF)\n",
    "review_baru_tfidf = [\n",
    "    'Saya benci film ini', # Harusnya 'Negatif'\n",
    "    'film ini bagus dan saya suka' # Harusnya 'Positif'\n",
    "]\n",
    "\n",
    "# 5. Prediksi (TF-IDF)\n",
    "# Gunakan .transform() saja\n",
    "X_baru_tfidf = tfidf_vectorizer.transform(review_baru_tfidf)\n",
    "\n",
    "# Lakukan prediksi\n",
    "prediksi_tfidf = model_tfidf.predict(X_baru_tfidf)\n",
    "\n",
    "print(\"Hasil Prediksi Model TF-IDF:\")\n",
    "print(f\"Review: {review_baru_tfidf[0]} -> Prediksi: {prediksi_tfidf[0]}\")\n",
    "print(f\"Review: {review_baru_tfidf[1]} -> Prediksi: {prediksi_tfidf[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igWPjeYxtD_C"
   },
   "source": [
    "## Bagian 3: Kesimpulan\n",
    "\n",
    "Saya telah berhasil membuat dua model Analisis Sentimen sederhana menggunakan dua metode vektorisasi yang berbeda:\n",
    "\n",
    "1.  **Bag-of-Words (BoW)**: Fokus pada **frekuensi** kata. Sangat efektif ketika dikombinasikan dengan n-grams (misal `(1, 2)`) untuk menangkap konteks seperti \"tidak suka\".\n",
    "2.  **TF-IDF**: Fokus pada **bobot/kepentingan** kata. Sangat efektif untuk menyaring kata-kata umum (seperti 'saya', 'ini') dan memberi nilai lebih pada kata yang unik namun penting (seperti 'bagus' atau 'benci').\n",
    "\n",
    "Kedua model berhasil memprediksi sentimen dari data uji dengan benar. Ini menunjukkan kedua teknik adalah dasar yang kuat untuk tugas-tugas NLP klasifikasi teks."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
